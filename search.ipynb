{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663270c2-ab2d-42b0-afd8-f05506eded3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wanglei/anaconda3/envs/wanglei/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home2/wanglei/anaconda3/envs/wanglei/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2024-07-04 15:40:29,221\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-04 15:40:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/home2/wanglei/Project/model/pythia2.8b_choose', tokenizer='/home2/wanglei/Project/model/pythia2.8b_choose', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-04 15:40:32 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n",
      "INFO 07-04 15:40:32 selector.py:25] Using XFormers backend.\n",
      "INFO 07-04 15:40:36 model_runner.py:104] Loading model weights took 5.1857 GB\n",
      "INFO 07-04 15:40:36 gpu_executor.py:94] # GPU blocks: 552, # CPU blocks: 819\n",
      "INFO 07-04 15:40:38 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-04 15:40:38 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-04 15:40:42 model_runner.py:867] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "import timeit\n",
    "import select\n",
    "import json\n",
    "from Lean4Gym import Lean4Gym, ProofState\n",
    "# print(torch.__version__)\n",
    "from torch.nn.parallel import DataParallel  \n",
    "# torch.cuda.set_device(2)\n",
    "\n",
    "from model import policy_model\n",
    "from model import value_model\n",
    "from trainer import Trainer\n",
    "from mcts import State\n",
    "from mcts import Node\n",
    "from mcts import MCTS\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c90883ad-7d37-421b-a3f2-59d191e76d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"/home2/wanglei/Project/model/pythia2.8b_choose\"\n",
    "tokenizer = transformers.GPTNeoXTokenizerFast.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20372810-b55c-4140-9ff3-49dd210e0ecf",
   "metadata": {},
   "source": [
    "å‚æ•°è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a15c7ec-156f-4b6b-a318-35ecfbe2aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu') \n",
    "\n",
    "args = {\n",
    "    \n",
    "    'batch_size': 4,\n",
    "    'numIters': 10,                                # Total number of training iterations\n",
    "    'num_simulations': 100,                         # Total number of MCTS simulations to run when deciding on a move to play\n",
    "    'numEps': 20,                                  # Number of full games (episodes) to run during each iteration\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "    'epochs': 10,                                    # Number of epochs of training per iteration\n",
    "    'checkpoint_path': 'latest.pth',                 # location to save latest set of weights\n",
    "    'TACRIC_NUMBER': 8,\n",
    "    'feature_size':100\n",
    "    # 'MAX_ROUND_NUMBER' : 10\n",
    "}\n",
    "\n",
    "feature_size = args['feature_size']  # ç‰¹å¾å‘é‡çš„size\n",
    "time_out = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3b0cf-ed8b-415c-9033-db9226be8dd0",
   "metadata": {},
   "source": [
    "å¯¼å…¥è®­ç»ƒå¥½çš„ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7fb362-bade-4e17-9fd0-cebdc9b9d1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyModel = policy_model(feature_size*2, device).to(device)\n",
    "valueModel = value_model(feature_size, device).to(device)\n",
    "\n",
    "checkpoint_policy = torch.load(\"/home2/wanglei/Project/alphazero_leanrepl_gpu/policy_model\")\n",
    "policyModel.load_state_dict(checkpoint_policy['state_dict'])\n",
    "\n",
    "checkpoint_value = torch.load(\"//home2/wanglei/Project/alphazero_leanrepl_gpu/value_model\")\n",
    "valueModel.load_state_dict(checkpoint_value['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cde4d-80fb-4d7d-986f-16292817925c",
   "metadata": {},
   "source": [
    "å¯¼å…¥å¾…è¯æ˜å®šç†ï¼š$ ğ¶_n^{nâˆ’k}=ğ¶_n^k   (k â‰¤ n) $ ,è·å–è¯¥å®šç†çŠ¶æ€å¹¶è®¾ç½®ä¸ºæ ¹èŠ‚ç‚¹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a053dc-ac91-4e7f-947f-7d0a01736e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lean_workdir = \"/home2/wanglei/Project\" # Leanå·¥ç¨‹çš„æ ¹ç›®å½•\n",
    "lean_file = \"demo.lean\"   # å¾…è¯æ˜å®šç†çš„Leanæ–‡ä»¶\n",
    "lean = Lean4Gym(lean_workdir, lean_file)\n",
    "state = lean.getInitState()\n",
    "init_state = State(state)\n",
    "node = Node()\n",
    "node.set_state(init_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11100c4-8860-4205-a741-0a89a3ca4a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æœç´¢ç­–ç•¥\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 8480346000156, Q/N: 0.0/0, state: <mcts.State object at 0x7b67c02cbb20>\n",
      "æˆåŠŸç­–ç•¥ï¼š\n",
      "æˆåŠŸè·¯å¾„ç­–ç•¥ï¼š\n",
      "rw [choose_eq_factorial_div_factorial hk]\n",
      "rw [choose_eq_factorial_div_factorial]\n",
      "rw [Nat.sub_sub_self hk]\n",
      "rw [mul_comm]\n",
      "exact Nat.sub_le _ _\n",
      "æˆåŠŸè·¯å¾„çŠ¶æ€ï¼š\n",
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ Nat.choose n (n - k) = Nat.choose n k\n",
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ Nat.choose n (n - k) = n ! / (k ! * (n - k)!)\n",
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n ! / ((n - k)! * (n - (n - k))!) = n ! / (k ! * (n - k)!)\n",
      "\n",
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n - k â‰¤ n\n",
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n ! / ((n - k)! * k !) = n ! / (k ! * (n - k)!)\n",
      "\n",
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n - k â‰¤ n\n",
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n - k â‰¤ n\n",
      "no goals\n",
      "æ‰€ç”¨æ—¶é—´ï¼šTrue\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "print(\"å¼€å§‹æœç´¢ç­–ç•¥\")\n",
    "mcts = MCTS(node, policyModel, valueModel, args, device)\n",
    "timespend = mcts.runmcts(lean, time_out)\n",
    "\n",
    "end = timeit.default_timer()\n",
    "\n",
    "print (\"æ‰€ç”¨æ—¶é—´ï¼š{}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df09ad4-7bd3-4ef5-9a5a-e00c2382a10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ Nat.choose n (n - k) = n ! / (k ! * (n - k)!)\n"
     ]
    }
   ],
   "source": [
    "state1 = lean.run_tactic(state, [\" rw[choose_eq_factorial_div_factorial hk]\"])\n",
    "print(state1.tacticState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "369ba1ac-c5ec-48c3-a155-230b102155c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n ! / ((n - k)! * (n - (n - k))!) = n ! / (k ! * (n - k)!)\n",
      "\n",
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n - k â‰¤ n\n"
     ]
    }
   ],
   "source": [
    "state2 = lean.run_tactic(state1, [\"rw[choose_eq_factorial_div_factorial]\"])\n",
    "print(state2.tacticState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63eadad8-3952-42ca-a849-c72bc6436278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n ! / ((n - k)! * k !) = n ! / (k ! * (n - k)!)\n",
      "\n",
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n - k â‰¤ n\n"
     ]
    }
   ],
   "source": [
    "state3 = lean.run_tactic(state2, [\"rw[Nat.sub_sub_self hk]\"])\n",
    "print(state3.tacticState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd98b354-f375-4aec-a5f9-9ead13c7a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n k : â„•\n",
      "hk : k â‰¤ n\n",
      "âŠ¢ n - k â‰¤ n\n"
     ]
    }
   ],
   "source": [
    "state4 = lean.run_tactic(state3, [\"rw [mul_comm]\"])\n",
    "print(state4.tacticState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc602f1-6384-4b54-9cf8-ac01a30a75af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no goals\n"
     ]
    }
   ],
   "source": [
    "state5 = lean.run_tactic(state4, [\"exact Nat.sub_le _ _\"])\n",
    "print(state5.tacticState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3356ef0c-cffe-4bd5-b913-4dff78b37947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
